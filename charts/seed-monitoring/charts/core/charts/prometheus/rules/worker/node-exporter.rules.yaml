groups:
- name: node-exporter.rules
  rules:
  - alert: NodeExporterDown
    expr: absent(up{job="node-exporter"} == 1)
    for: 1h
    labels:
      service: node-exporter
      severity: warning
      type: shoot
      visibility: owner
    annotations:
      summary: NodeExporter down or unreachable
      description: The NodeExporter has been down or unreachable from Prometheus for more than 1 hour.

  - alert: K8SNodeOutOfDisk
    expr: kube_node_status_condition{condition="OutOfDisk", status="true"} == 1
    for: 1h
    labels:
      service: node-exporter
      severity: critical
      type: shoot
      visibility: owner
    annotations:
      summary: Node ran out of disk space.
      description: Node {{ $labels.node }} has run out of disk space.

  - alert: K8SNodeMemoryPressure
    expr: kube_node_status_condition{condition="MemoryPressure", status="true"} == 1
    for: 1h
    labels:
      service: node-exporter
      severity: warning
      type: shoot
      visibility: owner
    annotations:
      summary: Node is under memory pressure.
      description: Node {{ $labels.node }} is under memory pressure.

  - alert: K8SNodeDiskPressure
    expr: kube_node_status_condition{condition="DiskPressure", status="true"} == 1
    for: 1h
    labels:
      service: node-exporter
      severity: warning
      type: shoot
      visibility: owner
    annotations:
      summary: Node is under disk pressure.
      description: Node {{ $labels.node }} is under disk pressure

  - record: instance:conntrack_entries_usage:percent
    expr: (node_nf_conntrack_entries / node_nf_conntrack_entries_limit) * 100

  # alert if the root filesystem is full
  - alert: VMRootfsFull
    expr: node_filesystem_free{mountpoint="/"} < 1024
    for: 1h
    labels:
      service: node-exporter
      severity: critical
      type: shoot
      visibility: owner
    annotations:
      description: Root filesystem device on instance {{ $labels.instance }} is almost full.
      summary: Node's root filesystem is almost full

  - alert: VMConntrackTableFull
    for: 1h
    expr: instance:conntrack_entries_usage:percent > 90
    labels:
      service: node-exporter
      severity: critical
      type: shoot
      visibility: owner
    annotations:
      description: The nf_conntrack table is {{ $value }}% full.
      summary: Number of tracked connections is near the limit

  - record: shoot:kube_node_info:count
    expr: count(kube_node_info{type="shoot"})

  # This recording rule creates a series for nodes with less than 5% free inodes on a not read only mount point.
  # The series exists only if there are less than 5% free inodes,
  # to keep the cardinality of these federated metrics manageable.
  # Otherwise we would get a series for each node in each shoot in the federating Prometheus.
  - record: shoot:node_filesystem_files_free:percent
    expr: |
      sum by (node, mountpoint)
        (node_filesystem_files_free / node_filesystem_files * 100 < 5
         and node_filesystem_readonly == 0)
