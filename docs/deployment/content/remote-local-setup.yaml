apiVersion: apps/v1
kind: Deployment
metadata:
  name: remote-local-setup
  labels: {component: remote-local-setup}
spec:
  selector:
    matchLabels: {component: remote-local-setup}
  strategy: {type: Recreate}
  template:
    metadata:
      labels: {component: remote-local-setup}
    spec:
      terminationGracePeriodSeconds: 1
      containers:
      - name: dev
        image: docker:20.10-dind
        command:
        - /bin/sh
        - -c
        - |
          set -ex
          cd
          apk add bash bash-completion curl fzf g++ git jq less lsof make mandoc mc procps tmux tmux-doc yq vim go
          apk add --repository=http://dl-cdn.alpinelinux.org/alpine/edge/main mount
          apk add --repository=http://dl-cdn.alpinelinux.org/alpine/edge/k9s k9s
          echo golang            && curl -sLO "https://go.dev/dl/$(curl -sL https://golang.org/VERSION?m=text).linux-amd64.tar.gz" && tar -C /usr/local -xzf go1.*.linux-amd64.tar.gz
          echo helm              && curl -sLO "https://get.helm.sh/helm-$(curl -sL https://api.github.com/repos/helm/helm/releases/latest | jq .tag_name -r)-linux-amd64.tar.gz" && tar -xzf helm-*-linux-amd64.tar.gz && mv linux-amd64/helm /usr/local/bin/helm
          echo kind              && curl -sL  "https://kind.sigs.k8s.io/dl/$(curl -sL https://api.github.com/repos/kubernetes-sigs/kind/releases/latest | jq .tag_name -r)/kind-linux-amd64" -o /usr/local/bin/kind && chmod +x /usr/local/bin/kind
          echo kns               && curl -sL  "https://raw.githubusercontent.com/blendle/kns/master/bin/kns" -o /usr/local/bin/kns && chmod +x /usr/local/bin/kns
          echo ktx               && curl -sL  "https://raw.githubusercontent.com/blendle/kns/master/bin/ktx" -o /usr/local/bin/ktx && chmod +x /usr/local/bin/ktx
          echo kube-ps1          && curl -sL  "https://raw.githubusercontent.com/jonmosco/kube-ps1/master/kube-ps1.sh" -o ~/.kube-ps1.sh
          echo kubectl           && curl -sL  "https://dl.k8s.io/release/$(curl -sL https://dl.k8s.io/release/stable.txt)/bin/linux/amd64/kubectl" -o /usr/local/bin/kubectl && chmod +x /usr/local/bin/kubectl
          echo tmux-completion   && curl -sL  "https://raw.githubusercontent.com/imomaliev/tmux-bash-completion/master/completions/tmux" -o /usr/share/bash-completion/completions/tmux
          echo docker-completion && curl -sL  "https://raw.githubusercontent.com/docker/docker-ce/master/components/cli/contrib/completion/bash/docker" -o /usr/share/bash-completion/completions/docker
          bash -c "echo api.{e2e-managedseed.garden,{local,e2e-{managedseed,hib,wl-hib,unpriv,wake-up,wl-wake-up,migrate,wl-migrate,rotate,wl-rotate,default,wl-default,upgrade-node,upgrade-zone,upd-node,wl-upd-node,upd-zone,wl-upd-zone,upgrade}}.local}.{internal,external}.local.gardener.cloud \
                   | sed 's/ /\n/g' | sed 's/^/127.0.0.1 /' | sort >> /etc/hosts"
          echo 'source ~/.bashrc' > ~/.bash_profile
          cat > ~/.bashrc <<"EOF"
            export PATH=$PATH:/usr/local/go/bin
            export KUBECONFIG=~/gardener/example/gardener-local/kind/local/kubeconfig:/tmp/kubeconfig-shoot-local.yaml
            source <(kubectl completion bash)
            alias k=kubectl
            complete -o default -F __start_kubectl k
            source ~/.kube-ps1.sh
            export PS1='[\w $(printf "$(kube_ps1)")]\$ '
            export TERM=xterm-256color
            cd ~/gardener
            echo -e "\e[1;33mTo attach to the \e[1;36mgardener\e[1;33m tmux session, run '\e[1;32mtmux attach -t gardener\e[1;33m'\e[0m"
          EOF
          cat > ~/.tmux.conf <<"EOF"
            set -g mouse on
            set -g mode-keys vi
            set -g default-shell /bin/bash
            set -g pane-border-status top
            set -g pane-border-format " #{pane_index} #{pane_title} - #{pane_current_command} "
          EOF
          git clone -q https://github.com/gardener/gardener.git
          dockerd-entrypoint.sh &
          until docker ps >/dev/null 2>&1; do sleep 1; done
          tmux new -d -s gardener -n gardener
          tmux select-pane -T top
          tmux send top Enter; sleep 1; tmux send 1; sleep 1; tmux send C; sleep 1; tmux send i; sleep 1; tmux send t; sleep 1; tmux send V; sleep 1; tmux send s; sleep 1; tmux send 5 Enter
          tmux split-window \; select-pane -T kind     ; sleep 1; tmux send "make kind-up # Set up KinD cluster (Garden and Seed)" \; select-layout even-vertical
          tmux split-window \; select-pane -T gardener ; sleep 1; tmux send "make gardener-up # Set up Gardener"\; select-layout even-vertical
          tmux split-window \; select-pane -T shoot    ; sleep 1; tmux send "kubectl apply -f example/provider-local/shoot.yaml # Create a new shoot cluster "\; select-layout even-vertical
          tmux split-window \; select-pane -T config   ; sleep 1; tmux send "kubectl -n garden-local get secret local.kubeconfig -o jsonpath={.data.kubeconfig} | base64 -d > /tmp/kubeconfig-shoot-local.yaml" \; select-layout even-vertical
          tmux split-window \; select-pane -T config   ; sleep 1; tmux send "make test-e2e-local-simple" # Run simple e2e test (create and delete shoot) \; select-layout even-vertical
          touch /tmp/ready
          read
        stdin: true
        startupProbe:
          exec:
            command:
            - cat
            - /tmp/ready
          failureThreshold: 100
          periodSeconds: 5
        resources:
          requests: {cpu: 8, memory: 8G}
          limits:   {cpu: 8, memory: 8G}
        securityContext:
          privileged: true
        volumeMounts:
        # Without bind mounting `/sys/fs/cgroup` the shoot worker node fails currently; all the other components work fine
        # Due to bind mounting `/sys/fs/cgroup` from the host, the docker container in this dind pod (i.e. the KinD cluster) uses a top level cgroup and hence is not constrained by the resource limits of this pod
        # These host cgroups might leak, but it is probably not an issue e.g. due to hibernating the hosting Gardener dev k8s cluster so that the nodes are recreated regularly anyway
        # To avoid conflicts on the top level docker cgroup, one dev pod per node is recommended
        # See
        # https://github.com/kubernetes-sigs/kind/issues/303
        # https://github.com/kubernetes/test-infra/blob/dcf27e157932c3e8680be4ae6cb8a4e2c7acf8cf/config/prow/config.yaml#L978-L988
        # https://github.com/gardener/ci-infra/blob/dff565bced0f386dd1acb0743beb3831dae6c10d/config/prow/config.yaml#L288-L298
        - {name: cgroup,  mountPath: /sys/fs/cgroup}
        - {name: modules, mountPath: /lib/modules, readOnly: true}
      volumes:
      - {name: cgroup,  hostPath: {type: Directory, path: /sys/fs/cgroup}}
      - {name: modules, hostPath: {type: Directory, path: /lib/modules}}
